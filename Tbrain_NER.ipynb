{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import re\n",
    "import os\n",
    "import torch\n",
    "from transformers import  AdamW\n",
    "from transformers import BertTokenizer, BertConfig, BertModel, BertPreTrainedModel, BertForTokenClassification\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import ast\n",
    "\n",
    "from ckiptagger import WS, POS, NER\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3893 977\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>news_ID</th>\n",
       "      <th>hyperlink</th>\n",
       "      <th>content</th>\n",
       "      <th>name</th>\n",
       "      <th>full_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>https://news.cnyes.com/news/id/4352432</td>\n",
       "      <td>0理財基金量化交易追求絕對報酬有效對抗牛熊市鉅亨網記者鄭心芸2019/07/05 22:35...</td>\n",
       "      <td>[]</td>\n",
       "      <td>近年來投資市場波動越來越明顯，追求低波動、絕對報酬的量化交易備受注目。專家表示，採用量化交易...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>https://udn.com/news/story/120775/4112519</td>\n",
       "      <td>10月13日晚間發生Uber Eats黃姓外送人員職災死亡案件 ### 省略內文 ### 北...</td>\n",
       "      <td>[]</td>\n",
       "      <td>\\r\\r\\n\\r\\r\\n\\r\\r\\n10月13日晚間發生Uber Eats黃姓外送人員職災死...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  news_ID                                  hyperlink  \\\n",
       "0           0        1     https://news.cnyes.com/news/id/4352432   \n",
       "1           1        2  https://udn.com/news/story/120775/4112519   \n",
       "\n",
       "                                             content name  \\\n",
       "0  0理財基金量化交易追求絕對報酬有效對抗牛熊市鉅亨網記者鄭心芸2019/07/05 22:35...   []   \n",
       "1  10月13日晚間發生Uber Eats黃姓外送人員職災死亡案件 ### 省略內文 ### 北...   []   \n",
       "\n",
       "                                        full_content  \n",
       "0  近年來投資市場波動越來越明顯，追求低波動、絕對報酬的量化交易備受注目。專家表示，採用量化交易...  \n",
       "1  \\r\\r\\n\\r\\r\\n\\r\\r\\n10月13日晚間發生Uber Eats黃姓外送人員職災死...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('EuSun_train.csv')\n",
    "val_df = pd.read_csv('EuSun_valid.csv')\n",
    "print(len(train_df),len(val_df))\n",
    "train_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\r\\r\\n\\r\\r\\n檢調偵辦國安局人員走私菸品案，陸續約談多名國安局人員，本週預計將會展開大規模動作，將遭民眾舉發的華航前資深副總經理羅雅美、前空品處副總邱彰信以及稱高層不知情的董事長兼總經理謝世謙列為貪汙罪被告，並盡速約談到案說明，揪出私菸案幕後「藏鏡人」。\\xa0私菸案爆發後，檢調已陸續約談吳宗憲、張恒嘉等多名國安局人員，據中國時報報導，由於有民眾告發，羅雅美、邱彰信2人被列為貪汙罪他字案被告，最快本週就會約談到案，揪出私菸案幕後「藏鏡人」。\\xa0另外，日前召開記者會代表華航說羅、邱2人對私菸案不知情的謝世謙，同樣因遭民眾告發涉案，也遭北檢簽分為他字案被告，預計將會是下波約談對象之一。最HOT話題在這！想跟上時事，快點我加入TVBS新聞LINE好友！\\r\\r\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['full_content'][3715]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['羅雅美', '邱彰信', '謝世謙', '吳宗憲', '張恒嘉']\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['name'][3715]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ckip\n",
    "# from ckiptagger import data_utils\n",
    "# data_utils.download_data_gdown(\"./\")\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "ws = WS(\"./data\", disable_cuda=False)\n",
    "pos = POS(\"./data\", disable_cuda=False)\n",
    "ner = NER(\"./data\", disable_cuda=False)\n",
    "# 18 entity types: https://github.com/ckiplab/ckiptagger/wiki/Entity-Types \n",
    "# 0 for others, 1 for mlp names\n",
    "entity_dict = {'GPE':2,'PERSON':3,'DATE':4,'ORG':5,'CARDINAL':6,\n",
    "'NORP':7,'LOC':8,'TIME':9,'FAC':10,'MONEY':11,'ORDINAL':12,'EVENT':13,\n",
    "'WORK_OF_ART':14,'QUANTITY':15,'PERCENT':16,'LANGUAGE':17,'PRODUCT':18,'LAW':19}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = train_df['full_content'][3715]\n",
    "# ws_results = ws([text])\n",
    "# pos_results = pos(ws_results)\n",
    "# ner_results = ner(ws_results, pos_results)\n",
    "# for n in ner_results[0]:\n",
    "#     print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, mode, tokenizer, data):\n",
    "        assert mode in [\"train\", \"test\"]  \n",
    "        self.mode = mode\n",
    "        self.data = data\n",
    "        self.len = len(self.data)\n",
    "        self.tokenizer = tokenizer  \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    def clean_text(self,text):\n",
    "        text = text.replace('\\r','')\n",
    "        text = text.replace('\\n','')\n",
    "        return text\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        content = self.data['full_content'][idx]\n",
    "        text = self.clean_text(content)\n",
    "        inputs = self.tokenizer.encode_plus(text=text, max_length=512, return_tensors='pt', \n",
    "                                            pad_to_max_length = True, \n",
    "                                            return_token_type_ids = True,\n",
    "                                            return_attention_mask=True)\n",
    "        \n",
    "        input_ids = inputs['input_ids'].squeeze(0)\n",
    "        segments_tensor = inputs['token_type_ids'].squeeze(0)\n",
    "        masks_tensor = inputs['attention_mask'].squeeze(0)\n",
    "        \n",
    "        if self.mode == 'train':\n",
    "            name_true = self.data['name'][idx] #list of \n",
    "            name_true = ast.literal_eval(name_true)\n",
    "            labels = [0]*512\n",
    "            \n",
    "            #ckip\n",
    "            ws_results = ws([text])\n",
    "            pos_results = pos(ws_results)\n",
    "            ner_results = ner(ws_results, pos_results)\n",
    "            for n in ner_results[0]:\n",
    "                label_start = int(n[0])\n",
    "                label_end = int(n[1])\n",
    "                if label_end <= 510:\n",
    "                    if n[2] == 'PERSON' and n[3] in name_true:\n",
    "                        label_num = 1\n",
    "                    else:\n",
    "                        label_num = entity_dict[n[2]]\n",
    "                    labels[label_start+1:label_end+1] = [label_num]* len(n[3])\n",
    "            return input_ids, torch.tensor(labels)\n",
    "            \n",
    "#             if not name_true:\n",
    "#                 return input_ids, torch.tensor(labels)\n",
    "#             else:\n",
    "#                 #print(idx,name_true)\n",
    "#                 for name in name_true:\n",
    "#                     index = [i.start() for i in re.finditer(name, text)]\n",
    "#                     for i in range(510-3):\n",
    "#                         if i in index:\n",
    "#                             labels[i+1:i+len(name)+1] = [1]*len(name)\n",
    "#                             i = i+len(name)\n",
    "#                 return input_ids, torch.tensor(labels)\n",
    "        else:\n",
    "            newsid = self.data['news_ID'][idx]\n",
    "            return newsid, input_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForTokenClassification(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = 20 ###\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = torch.nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = torch.nn.Linear(config.hidden_size, self.num_labels)\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "    ):\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0]\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        logits = self.classifier(sequence_output)\n",
    "\n",
    "        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n",
    "        if labels is not None:\n",
    "            loss_fct = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "            if attention_mask is not None:\n",
    "                active_loss = attention_mask.view(-1) == 1\n",
    "                active_logits = logits.view(-1, self.num_labels)\n",
    "                active_labels = torch.where(\n",
    "                    active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels)\n",
    "                )\n",
    "                loss = loss_fct(active_logits, active_labels)\n",
    "            else:\n",
    "                #print(input_ids.shape,logits.shape,sequence_output.shape)\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            outputs = (loss,) + outputs\n",
    "\n",
    "        return outputs  # (loss), scores, (hidden_states), (attentions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asMinutes(s): #s = time.time()-start_time\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def trainIter(PRETRAINED_MODEL_NAME, trainloader, epochs, LR):\n",
    "    config = BertConfig.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "    model = BertForTokenClassification.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "    \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.train().to(device)\n",
    "    min_dev_loss, best_epoch = 1000000, 0\n",
    "    start_time = time.time()\n",
    "    optimizer = AdamW(model.parameters(),lr = LR)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_steps = len(trainloader) * epochs\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=100, num_training_steps = total_steps)\n",
    "        \n",
    "        steps, train_loss = 0, 0\n",
    "        for data in trainloader:\n",
    "            input_ids, labels = [t.to(device) for t in data] \n",
    "            \n",
    "            outputs = model(input_ids, labels=labels)\n",
    "            loss = outputs[0]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            model.zero_grad()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            steps += 1\n",
    "            print(f'Epoch : {epoch+1}/{epochs}, setps:{steps}, time: {asMinutes(time.time()-start_time)}, Training Loss : {train_loss/steps}',  end = '\\r')\n",
    "        print('\\n===========================================================')\n",
    "        \n",
    "        torch.save(model.state_dict(), f'bert_{epoch}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BS = 6\n",
    "PRETRAINED_MODEL_NAME = \"bert-base-chinese\"\n",
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME, do_lower_case=True)\n",
    "train = NewsDataset('train',tokenizer, train_df)\n",
    "trainloader = torch.utils.data.DataLoader(train, batch_size=BS,drop_last = True,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data in trainloader:\n",
    "#     input_ids, labels = [t for t in data] \n",
    "#     #print(input_ids)\n",
    "#     for l in labels:\n",
    "#         print(l)\n",
    "#     #print(torch.nonzero(labels[0]))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1/10 ,setps:257,time: 70m 23s, Training Loss : 0.8921828425348037\r"
     ]
    }
   ],
   "source": [
    "trainIter(PRETRAINED_MODEL_NAME, trainloader,epochs=10, LR=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(tokenizer, test_loader,model):\n",
    "    prediction = pd.DataFrame(columns=['news_ID','name_pred'])\n",
    "    with torch.no_grad():\n",
    "        ids, pred = [], []\n",
    "        for data in test_loader: \n",
    "            newsid, input_ids = [t.to(device) for t in data] \n",
    "            outputs = model(input_ids)\n",
    "            ans_pred = torch.sigmoid(outputs[0])\n",
    "            _, ans_pred = ans_pred.max(-1)\n",
    "            #print(ans_pred)\n",
    "            ans_pred = list(ans_pred[0])\n",
    "            ans_index = [i for i,x in enumerate(ans_pred) if x == 1]\n",
    "            #print(ans_index)\n",
    "            text = [tokenizer.convert_ids_to_tokens(i) for i in input_ids]\n",
    "            text = list(text[0])\n",
    "            ans_tokens = [text[x] for i,x in enumerate(ans_index)]\n",
    "            #print(ans_tokens)\n",
    "            names, answer = [], []\n",
    "            if ans_tokens != []:\n",
    "                ws_results = ws([text])\n",
    "                pos_results = pos(ws_results)\n",
    "                ner_results = ner(ws_results, pos_results)\n",
    "                for n in ner_results[0]:\n",
    "                    if n[2] == 'PERSON':\n",
    "                        names.append(n[3])\n",
    "            #print(newsid,names)\n",
    "            for t in ans_tokens:\n",
    "                for n in names:\n",
    "                    if t in n and n not in answer and len(n) > 2:\n",
    "                        if len(n) < 4:\n",
    "                            answer.append(n)\n",
    "            ids.append(newsid)\n",
    "            pred.append(answer)\n",
    "            print(f'news id : {newsid.item()}, names : {answer}', end = '\\r')\n",
    "        prediction['news_ID'] = ids\n",
    "        prediction['name_pred'] = pred\n",
    "    return prediction\n",
    "        \n",
    "def get_score(name_preds, true_preds):\n",
    "    total_score=0\n",
    "    no_names = 0\n",
    "    for i in range(len(name_preds)):\n",
    "        name_pred = name_preds[i]\n",
    "        true_pred = true_preds[i]\n",
    "        if name_pred == [] and true_pred == []:\n",
    "            total_score += 1\n",
    "            no_names += 1\n",
    "        elif name_pred != [] and true_pred != []:\n",
    "            #print(name_pred,true_pred, set(name_pred) & set(true_pred))\n",
    "            intersection = list(set(name_pred) & set(true_pred))\n",
    "            recall = len(intersection)/len(true_pred)\n",
    "            precision = len(intersection)/len(name_pred)\n",
    "            if recall != 0 and precision != 0:\n",
    "                f1 = 2/(1/recall+1/precision) \n",
    "            else:\n",
    "                f1 = 0\n",
    "            total_score += f1\n",
    "    return no_names, total_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "news id : 4953, names : ['許玉秀', '王隆昌'] '廖麗櫻'] '吳清吉', '蔡清華', '連定安', '鍾葦怡']\r"
     ]
    }
   ],
   "source": [
    "config = BertConfig.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "model = BertForTokenClassification(config)       \n",
    "model = model.cuda()\n",
    "checkpoint = torch.load('bert_3')\n",
    "model.load_state_dict(checkpoint)\n",
    "model = model.eval()\n",
    "BS=1\n",
    "val = NewsDataset('test',tokenizer, val_df)\n",
    "valloader = torch.utils.data.DataLoader(val, batch_size=BS,shuffle=False)\n",
    "prediction = evaluation(tokenizer, valloader,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of empty names: 907\n",
      "total score: 947.8582251082247\n"
     ]
    }
   ],
   "source": [
    "true_pred = [ast.literal_eval(i) for i in val_df['name']]\n",
    "name_pred = prediction['name_pred'].to_list()\n",
    "no_names, total_score = get_score(name_pred,true_pred)\n",
    "print('num of empty names:', no_names)\n",
    "print('total score:', total_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33, 35, 'DATE', '本週')\n",
      "(63, 66, 'PERSON', '羅雅美')\n",
      "(73, 76, 'PERSON', '邱彰信')\n",
      "(236, 237, 'PERSON', '邱')\n",
      "(68, 71, 'ORG', '空品處')\n",
      "(173, 176, 'PERSON', '羅雅美')\n",
      "(10, 13, 'ORG', '國安局')\n",
      "(158, 162, 'WORK_OF_ART', '中國時報')\n",
      "(196, 198, 'DATE', '本週')\n",
      "(222, 224, 'DATE', '日前')\n",
      "(234, 235, 'PERSON', '羅')\n",
      "(237, 238, 'CARDINAL', '2')\n",
      "(177, 180, 'PERSON', '邱彰信')\n",
      "(231, 233, 'ORG', '華航')\n",
      "(92, 95, 'PERSON', '謝世謙')\n",
      "(151, 154, 'ORG', '國安局')\n",
      "(309, 313, 'ORG', 'TVBS')\n",
      "(145, 148, 'PERSON', '張恒嘉')\n",
      "(247, 250, 'PERSON', '謝世謙')\n",
      "(54, 56, 'ORG', '華航')\n",
      "(141, 144, 'PERSON', '吳宗憲')\n",
      "(27, 30, 'ORG', '國安局')\n",
      "(180, 181, 'CARDINAL', '2')\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "r08725043",
   "language": "python",
   "name": "r08725043"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
