{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "labeling: tokenization problem \n",
    "\n",
    "XLNET: padding in the front\n",
    "\n",
    "CrossentropyLoss: ignore padding\n",
    "\n",
    "Prevent name tokenization errors: \n",
    "name = 123, \n",
    "if name12 already in output dont append,\n",
    "if "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import re\n",
    "import os\n",
    "import torch\n",
    "from transformers import  AdamW\n",
    "from transformers import BertTokenizer, BertConfig, BertModel, BertPreTrainedModel, BertForTokenClassification\n",
    "from transformers import XLNetTokenizer, XLNetConfig, XLNetModel, XLNetPreTrainedModel, XLNetForTokenClassification\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import ast\n",
    "#from sklearn.cross_validation import train_test_split\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "nlp = StanfordCoreNLP('stanford-corenlp-full-2018-10-05/', lang='zh')\n",
    "from ckiptagger import WS, POS, NER\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = pd.read_csv('EuSun_train.csv')\n",
    "# test_df = pd.read_csv('EuSun_valid.csv')\n",
    "# print(len(train_df),len(test_df))\n",
    "# train_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>content</th>\n",
       "      <th>full_content</th>\n",
       "      <th>hyperlink</th>\n",
       "      <th>link</th>\n",
       "      <th>name</th>\n",
       "      <th>news_ID</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0理財基金量化交易追求絕對報酬有效對抗牛熊市鉅亨網記者鄭心芸2019/07/05 22:35...</td>\n",
       "      <td>近年來投資市場波動越來越明顯，追求低波動、絕對報酬的量化交易備受注目。專家表示，採用量化交易...</td>\n",
       "      <td>https://news.cnyes.com/news/id/4352432</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                            content  \\\n",
       "0           0  0理財基金量化交易追求絕對報酬有效對抗牛熊市鉅亨網記者鄭心芸2019/07/05 22:35...   \n",
       "\n",
       "                                        full_content  \\\n",
       "0  近年來投資市場波動越來越明顯，追求低波動、絕對報酬的量化交易備受注目。專家表示，採用量化交易...   \n",
       "\n",
       "                                hyperlink link name news_ID title  \n",
       "0  https://news.cnyes.com/news/id/4352432  NaN   []       1   NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('aug_updated_EuSun_train_0718.csv')\n",
    "train_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5886 683\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>LINK</th>\n",
       "      <th>full_content</th>\n",
       "      <th>name</th>\n",
       "      <th>news_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://mops.twse.com.tw/mops/web/t05st02?step...</td>\n",
       "      <td>1.董事會決議日期或發生變動日期:107/09/21\\n\\n2.人員別（請輸入董事長或總經理...</td>\n",
       "      <td>[]</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.hk01.com/社會新聞/383851/逃犯條例-建造業議會指立法...</td>\n",
       "      <td>建造業議會指，2018至2019財政年度建造工程完成量按年下跌8.4%。主席陳家駒表示，本港...</td>\n",
       "      <td>[]</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>https://news.mingpao.com/ins/%e6%b8%af%e8%81%9...</td>\n",
       "      <td>上午8時天文台錄得氣溫27度，相對濕度百分之75%。\\n\\n過去一小時，京士柏錄得的平均紫外...</td>\n",
       "      <td>[]</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1  \\\n",
       "0           0             0   \n",
       "1           1             1   \n",
       "2           2             2   \n",
       "\n",
       "                                                LINK  \\\n",
       "0  https://mops.twse.com.tw/mops/web/t05st02?step...   \n",
       "1  https://www.hk01.com/社會新聞/383851/逃犯條例-建造業議會指立法...   \n",
       "2  https://news.mingpao.com/ins/%e6%b8%af%e8%81%9...   \n",
       "\n",
       "                                        full_content name news_ID  \n",
       "0  1.董事會決議日期或發生變動日期:107/09/21\\n\\n2.人員別（請輸入董事長或總經理...   []      11  \n",
       "1  建造業議會指，2018至2019財政年度建造工程完成量按年下跌8.4%。主席陳家駒表示，本港...   []      12  \n",
       "2  上午8時天文台錄得氣溫27度，相對濕度百分之75%。\\n\\n過去一小時，京士柏錄得的平均紫外...   []      20  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('final_train_0.9.csv')\n",
    "test_df = pd.read_csv('final_valid_0.9.csv')\n",
    "print(len(train_df),len(test_df))\n",
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>LINK</th>\n",
       "      <th>full_content</th>\n",
       "      <th>name</th>\n",
       "      <th>news_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>https://www.mirrormedia.mg/story/20191223web003/</td>\n",
       "      <td>馬雲21日在世界浙商上海論壇暨上海市浙江商會年會談話，談到做生意其實每一年日子都不容易，但是...</td>\n",
       "      <td>[]</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1                                              LINK  \\\n",
       "0           0             8  https://www.mirrormedia.mg/story/20191223web003/   \n",
       "\n",
       "                                        full_content name news_ID  \n",
       "0  馬雲21日在世界浙商上海論壇暨上海市浙江商會年會談話，談到做生意其實每一年日子都不容易，但是...   []      69  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols_to_use = ['news_ID', 'hyperlink','name','full_content']\n",
    "# new_data = pd.read_csv('new_data.csv', usecols= cols_to_use)\n",
    "# new_data.head(3)\n",
    "# frames = [train_df,new_data]\n",
    "# train_df = pd.concat(frames,ignore_index=True)\n",
    "# train_df.drop(train_df.columns[0], axis=1)\n",
    "# print(len(train_df))\n",
    "# train_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4794"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['name'].to_list().count('[]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'香港特首林鄭月娥週二 (9 日) 表示，擬議的逃犯條例法案已「壽終正寢」，修訂《逃犯條例》工作已徹底全面停止，希望香港市民不要因用不同字眼而有不同理解。\\n\\n林鄭月娥指出，社會的矛盾、紛爭、不滿及憤怒，都是因特區政府提出修訂《逃犯條例》而引起，政府今次修例工作完全失敗，包括對於社會脈搏掌握不夠，對政治敏感度有偏差，構成這次的失敗。\\n\\n過去數週以來，香港爆發了歷史性的遊行與抗議活動，林鄭曾一直強調修法的正常及必要性，拒絕撤回法案，也引來香港民眾一直存有疑慮，抗爭不斷。這次她仍未明言「撤回」，但確認法案已「壽終正寢」。\\n\\n林鄭指出，由這次的失敗，會加強行會成員收集意見的功能，包括行會成員應各自負責接觸不同群體，然後向特首反映各界意見。她還表示，願意與學生代表進行不設前提的公開對話。\\n\\n林鄭說，從 5 年前的佔中到今日反修例，都反映社會有很多深層次問題，政府不可以視而不見，應找出問題癥結，對症下藥，期盼可以修補社會撕裂。\\n\\n林鄭表示，她對香港人的素質感到自豪，絕大多數抗議者的和平行為表明了這一點。但是，她仍指責，極少數抗議者利用這種機會訴諸暴力及破壞，她很遺憾見到這些暴力行為，因為這破壞了香港的法治。\\n\\n她強調，要求大赦的想法，或是不調查及起訴示威期間違法的人，是「不可接受的」。\\n\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['full_content'][3715]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[]'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['name'][3715]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ckip\n",
    "# from ckiptagger import data_utils\n",
    "# data_utils.download_data_gdown(\"./\")\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "ws = WS(\"./data\", disable_cuda=False)\n",
    "# pos = POS(\"./data\", disable_cuda=False)\n",
    "# ner = NER(\"./data\", disable_cuda=False)\n",
    "# # 18 entity types: https://github.com/ckiplab/ckiptagger/wiki/Entity-Types \n",
    "# # 0 for others, 1 for mlp names\n",
    "# entity_dict = {'GPE':2,'PERSON':3,'DATE':4,'ORG':5,'CARDINAL':6,\n",
    "# 'NORP':7,'LOC':8,'TIME':9,'FAC':10,'MONEY':11,'ORDINAL':12,'EVENT':13,\n",
    "# 'WORK_OF_ART':14,'QUANTITY':15,'PERCENT':16,'LANGUAGE':17,'PRODUCT':18,'LAW':19}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, mode, tokenizer, data):\n",
    "        assert mode in [\"train\", \"test\"]  \n",
    "        self.mode = mode\n",
    "        self.data = data\n",
    "        self.len = len(self.data)\n",
    "        self.tokenizer = tokenizer  \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    def clean_text(self,text):\n",
    "        text = text.replace('\\r','')\n",
    "        text = text.replace('\\n','')\n",
    "        return text\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        MAX_LEN = 600\n",
    "        content = self.data['full_content'][idx]\n",
    "        text = self.clean_text(content)\n",
    "        inputs = self.tokenizer.encode_plus(text=text, max_length=MAX_LEN, return_tensors='pt', \n",
    "                                            pad_to_max_length = True, \n",
    "                                            return_token_type_ids = True,\n",
    "                                            return_attention_mask=True,\n",
    "                                           truncation=\"longest_first\",)\n",
    "        \n",
    "        input_ids = inputs['input_ids'].squeeze(0)\n",
    "        segments_tensor = inputs['token_type_ids'].squeeze(0)\n",
    "        masks_tensor = inputs['attention_mask'].squeeze(0)\n",
    "        \n",
    "        if self.mode == 'train':\n",
    "            name_true = self.data['name'][idx] #list of \n",
    "            name_true = ast.literal_eval(name_true)\n",
    "            tokens = self.tokenizer.tokenize(text)\n",
    "            #print(tokens)\n",
    "            labels = [4]*MAX_LEN\n",
    "            if len(tokens) < MAX_LEN:\n",
    "                PADDING_LEN = MAX_LEN - 2 - len(tokens)\n",
    "            else:\n",
    "                PADDING_LEN = 0\n",
    "            \n",
    "            if not name_true:\n",
    "                return input_ids, torch.tensor(labels)\n",
    "            else:\n",
    "                #print(idx,name_true)\n",
    "                for name in name_true:\n",
    "                    #index = [i.start() for i in re.finditer(name, text)]\n",
    "                    indexes = [i for i, e in enumerate(tokens) if e == name[0]]\n",
    "                    for i in indexes:\n",
    "                        if i < MAX_LEN-2-3:\n",
    "                            if ''.join(tokens[i:i+len(name)]) == name:\n",
    "                                for j in range(len(name)):\n",
    "                                    if j == 0:\n",
    "                                        labels[i+j+PADDING_LEN] = 1\n",
    "                                    elif j == len(name)-1:\n",
    "                                        labels[i+j+PADDING_LEN] = 3\n",
    "                                    else:\n",
    "                                        labels[i+j+PADDING_LEN] = 2\n",
    "                return input_ids, torch.tensor(labels)\n",
    "        else:\n",
    "            newsid = self.data['news_ID'][idx]\n",
    "            return newsid, input_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForTokenClassification(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = 4 ###\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = torch.nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = torch.nn.Linear(config.hidden_size, self.num_labels)\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "    ):\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0]\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        logits = self.classifier(sequence_output)\n",
    "\n",
    "        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n",
    "        if labels is not None:\n",
    "            #weight = [1-3631/(3893+3631), 1-(3893-3631)/(3893+3631)]\n",
    "            weight = [1-0.8,1-0.2/3,1-0.2/3,1-0.2/3]\n",
    "            class_weight = torch.FloatTensor(weight).to(device)\n",
    "            loss_fct = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "            if attention_mask is not None:\n",
    "                active_loss = attention_mask.view(-1) == 1\n",
    "                active_logits = logits.view(-1, self.num_labels)\n",
    "                active_labels = torch.where(\n",
    "                    active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels)\n",
    "                )\n",
    "                loss = loss_fct(active_logits, active_labels)\n",
    "            else:\n",
    "                #print(input_ids.shape,logits.shape,sequence_output.shape)\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            outputs = (loss,) + outputs\n",
    "\n",
    "        return outputs  # (loss), scores, (hidden_states), (attentions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XLNetForTokenClassification(XLNetPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = 5 ###\n",
    "        self.transformer = XLNetModel(config)\n",
    "        self.classifier = torch.nn.Linear(config.hidden_size, self.num_labels)\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        mems=None,\n",
    "        perm_mask=None,\n",
    "        target_mapping=None,\n",
    "        token_type_ids=None,\n",
    "        input_mask=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        use_cache=True,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "    ):\n",
    "\n",
    "        outputs = self.transformer(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            mems=mems,\n",
    "            perm_mask=perm_mask,\n",
    "            target_mapping=target_mapping,\n",
    "            token_type_ids=token_type_ids,\n",
    "           \n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0]\n",
    "\n",
    "        logits = self.classifier(sequence_output)\n",
    "        \n",
    "        outputs = (logits,) + outputs[1:]  # Keep mems, hidden states, attentions if there are in it\n",
    "        if labels is not None:\n",
    "            weight = [0.5,1-0.85,1-0.15/3,1-0.15/3,1-0.15/3]\n",
    "            class_weight = torch.FloatTensor(weight).to(device)\n",
    "            loss_fct = torch.nn.CrossEntropyLoss(ignore_index=0,weight=class_weight)\n",
    "            # Only keep active parts of the loss\n",
    "            if attention_mask is not None:\n",
    "                active_loss = attention_mask.view(-1) == 1\n",
    "                active_logits = logits.view(-1, self.num_labels)\n",
    "                active_labels = torch.where(\n",
    "                    active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels)\n",
    "                )\n",
    "                loss = loss_fct(active_logits, active_labels)\n",
    "            else:\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            outputs = (loss,) + outputs\n",
    "\n",
    "        return outputs  # return (loss), logits, (mems), (hidden states), (attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asMinutes(s): #s = time.time()-start_time\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def trainIter(MAX_LEN, PRETRAINED_MODEL_NAME, trainloader, valloader, epochs, LR):\n",
    "    #config = BertConfig.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "    #model = BertForTokenClassification.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "    config = XLNetConfig.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "    model = XLNetForTokenClassification.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "    \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.train().to(device)\n",
    "    min_dev_loss, best_epoch = 1000000, 0\n",
    "    start_time = time.time()\n",
    "    optimizer = AdamW(model.parameters(),lr = LR)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_steps = len(trainloader) * epochs\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=100, num_training_steps = total_steps)\n",
    "        \n",
    "        steps, train_loss = 0, 0\n",
    "        for data in trainloader:\n",
    "            input_ids, labels = [t.to(device) for t in data] \n",
    "            outputs = model(input_ids, labels=labels)\n",
    "            loss = outputs[0]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            model.zero_grad()\n",
    "            train_loss += loss.item()\n",
    "            steps += 1\n",
    "            print(f'Epoch : {epoch+1}/{epochs}, setps:{steps}, time: {asMinutes(time.time()-start_time)}, Training Loss : {train_loss/steps}',  end = '\\r')\n",
    "        print('\\n===========================================================')\n",
    "        \n",
    "        prediction = evaluation(MAX_LEN, tokenizer, valloader,model)\n",
    "        true_pred = [ast.literal_eval(i) for i in test_df['name']]\n",
    "        name_pred = prediction['name_pred'].to_list()\n",
    "        avg_f1, nonempty, actual_nonempty, no_names, actual_nonames, total_score = get_score(name_pred,true_pred)\n",
    "        print(f'Epoch : {epoch+1}/{epochs}, time: {asMinutes(time.time()-start_time)}, Validation Score : {total_score}',  end = '\\r')\n",
    "        print('\\n===========================================================')\n",
    "        \n",
    "        torch.save(model.state_dict(), f'xlnet_{int(total_score)}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BS = 6\n",
    "# PRETRAINED_MODEL_NAME = \"bert-base-chinese\"\n",
    "# tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME, do_lower_case=True)\n",
    "\n",
    "# # trainsize = int(0.9*len(train_df))\n",
    "# # validsize = len(train_df) - trainsize\n",
    "# # train_df, val_df = torch.utils.data.random_split(train_df,[trainsize, validsize])\n",
    "\n",
    "# train = NewsDataset('train',tokenizer, train_df)\n",
    "# trainloader = torch.utils.data.DataLoader(train, batch_size=BS,drop_last = True,shuffle=True)\n",
    "# #val = NewsDataset('train',tokenizer, val_df)\n",
    "# #valloader = torch.utils.data.DataLoader(val, batch_size=BS,drop_last = True,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BS = 6\n",
    "PRETRAINED_MODEL_NAME = \"hfl/chinese-xlnet-base\"\n",
    "tokenizer = XLNetTokenizer.from_pretrained(PRETRAINED_MODEL_NAME, do_lower_case=True)\n",
    "train = NewsDataset('train',tokenizer, train_df)\n",
    "trainloader = torch.utils.data.DataLoader(train, batch_size=BS,drop_last = True,shuffle=True)\n",
    "test = NewsDataset('test',tokenizer, test_df)\n",
    "testloader = torch.utils.data.DataLoader(test, batch_size=1,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data in testloader:\n",
    "    \n",
    "#     input_ids =  [i.to(device) for i in data[1]][0]\n",
    "#     input_ids = input_ids.unsqueeze(0)\n",
    "#     news_id = data[0]\n",
    "#     print(news_id,input_ids.shape)\n",
    "# #     input_ids, labels = [t.to(device) for t in data] \n",
    "# #     print(input_ids,labels.shape)\n",
    "# # #     for l in labels:\n",
    "# # #         print(l)\n",
    "# #     #print(torch.nonzero(labels[0]))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(MAX_LEN, tokenizer, test_loader,model):\n",
    "    prediction = pd.DataFrame(columns=['news_ID','name_pred'])\n",
    "    fil = re.compile(u'[^0-9a-zA-Z\\u4e00-\\u9fa5.，,。？“”]+', re.UNICODE)\n",
    "    with torch.no_grad():\n",
    "        ids, pred = [], []\n",
    "        for data in test_loader: \n",
    "            input_ids =  [i.to(device) for i in data[1]][0]\n",
    "            input_ids = input_ids.unsqueeze(0)\n",
    "            newsid = data[0]\n",
    "            outputs = model(input_ids)\n",
    "            ans_pred = torch.sigmoid(outputs[0])\n",
    "            _, ans_pred = ans_pred.max(-1)\n",
    "            #print(ans_pred)\n",
    "            ans_pred = list(ans_pred[0])\n",
    "            #ans_index = [i for i,x in enumerate(ans_pred) if x ==2]\n",
    "            text = [tokenizer.convert_ids_to_tokens(i) for i in input_ids]\n",
    "            text = list(text[0])\n",
    "            \n",
    "            PADDING_LEN = text.count('<pad>')\n",
    "            ans_index = [i for i in range(PADDING_LEN+2,len(ans_pred)) if ans_pred[i] ==1]\n",
    "            #print(ans_index)         \n",
    "            ans_tokens = [text[x] for i,x in enumerate(ans_index)]\n",
    "            #ans_tokens = [text[ans_index[i]]+text[ans_index[i]+1] for i in range(len(ans_index)-1)]\n",
    "            \n",
    "            ans_tokens=[]\n",
    "            a = ''\n",
    "            is_ans = False\n",
    "            for i in range(len(ans_index)):\n",
    "                index = ans_index[i]\n",
    "                if ans_pred[index].item() == 1 and text[index] != '▁':\n",
    "                    a = text[index]\n",
    "                    for j in range(1,4):\n",
    "                        #print(text[ans_index[i]],ans_index[i],ans_pred[ans_index[i]])\n",
    "                        if ans_pred[index+j].item() == 3: \n",
    "                            a += text[index+j]\n",
    "                            #print('1',a)\n",
    "                            is_ans = True\n",
    "                            break\n",
    "                        else:\n",
    "                            if index+j > MAX_LEN-3:\n",
    "                                is_ans = False\n",
    "                                break\n",
    "                            else:\n",
    "                                a += text[index+j]  \n",
    "                if is_ans:\n",
    "                    ans_tokens.append(fil.sub('', a))\n",
    "                \n",
    "            #ans_tokens = [text[ans_index[i]]+text[ans_index[i]+1]+text[ans_index[i]+2] for i in range(len(ans_index)-1)]\n",
    "            \n",
    "            names, answer = [], []\n",
    "#             if ans_tokens != []:\n",
    "#                 #print(ans_tokens)\n",
    "#                 ws_results = ws([text])\n",
    "#                 pos_results = pos(ws_results)\n",
    "#                 ner_results = ner(ws_results, pos_results)\n",
    "#                 #count word frequency\n",
    "#                 word_dict = dict()\n",
    "#                 for w in ws_results[0]:\n",
    "#                     if w not in list(word_dict.keys()):\n",
    "#                         word_dict[w] = 1 \n",
    "#                     else:\n",
    "#                         word_dict[w] += 1 \n",
    "#                 for n in ner_results[0]:\n",
    "#                     if n[2] == 'PERSON' and len(n[3]) < 5:\n",
    "#                         names.append(n[3])\n",
    "#             names = list(set(names))\n",
    "#             names_2 = [x for x in names if len(x) == 2]\n",
    "            #print(newsid,names)\n",
    "            \n",
    "            for t in ans_tokens:\n",
    "                if t not in answer and len(t) < 5 and len(t) > 1:\n",
    "                        if len(t) == 2 and t[1] in ['嫌','男','女','妻']:\n",
    "                            print('discard',t)\n",
    "                        elif len(t) == 4 and t[0]+t[1]+t[2] in names:\n",
    "                            print('discard',t)\n",
    "                        else:\n",
    "                            answer.append(t)\n",
    "                         \n",
    "                        \n",
    "            ids.append(newsid)\n",
    "            pred.append(answer)\n",
    "            print(f'news id : {newsid}, names : {answer}', end = '\\r')\n",
    "        prediction['news_ID'] = ids\n",
    "        prediction['name_pred'] = pred\n",
    "    return prediction\n",
    "        \n",
    "def get_score(name_preds, true_preds):\n",
    "    total_score, no_names, actual_nonames = 0,0,0\n",
    "    nonempty, actual_nonempty = 0,0\n",
    "    total_f1 = 0\n",
    "    for i in range(len(name_preds)):\n",
    "        name_pred = name_preds[i]\n",
    "        true_pred = true_preds[i]\n",
    "        if true_pred == []:\n",
    "            actual_nonames +=1\n",
    "        else:\n",
    "            actual_nonempty += 1\n",
    "            \n",
    "        if name_pred == [] and true_pred == []:\n",
    "            total_score += 1\n",
    "            no_names += 1\n",
    "        elif name_pred != [] and true_pred != []:\n",
    "            #print(name_pred,true_pred, set(name_pred) & set(true_pred))\n",
    "            intersection = list(set(name_pred) & set(true_pred))\n",
    "            recall = len(intersection)/len(true_pred)\n",
    "            precision = len(intersection)/len(name_pred)\n",
    "            if recall != 0 and precision != 0:\n",
    "                f1 = 2/(1/recall+1/precision) \n",
    "            else:\n",
    "                f1 = 0\n",
    "            total_score += f1\n",
    "            total_f1+=f1\n",
    "            nonempty += 1\n",
    "    return total_f1/nonempty, nonempty, actual_nonempty, no_names, actual_nonames, total_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainIter(600, PRETRAINED_MODEL_NAME, trainloader, testloader, epochs=9, LR=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ckip_evaluation(MAX_LEN, tokenizer, test_loader,model):\n",
    "    prediction = pd.DataFrame(columns=['news_ID','name_pred'])\n",
    "    with torch.no_grad():\n",
    "        ids, pred = [], []\n",
    "        for data in test_loader: \n",
    "            input_ids =  [i.to(device) for i in data[1]][0]\n",
    "            input_ids = input_ids.unsqueeze(0)\n",
    "            newsid = data[0]\n",
    "            outputs = model(input_ids)\n",
    "            ans_pred = torch.sigmoid(outputs[0])\n",
    "            _, ans_pred = ans_pred.max(-1)\n",
    "            #print(ans_pred)\n",
    "            ans_pred = list(ans_pred[0])\n",
    "            #ans_index = [i for i,x in enumerate(ans_pred) if x ==2]\n",
    "            ans_index = [i for i in range(len(ans_pred)) if ans_pred[i] ==1]\n",
    "            #print(ans_index)\n",
    "            text = [tokenizer.convert_ids_to_tokens(i) for i in input_ids]\n",
    "            text = list(text[0])\n",
    "            ans_tokens = [text[x] for i,x in enumerate(ans_index)]\n",
    "            #ans_tokens = [text[ans_index[i]]+text[ans_index[i]+1] for i in range(len(ans_index)-1)]\n",
    "            \n",
    "            ans_tokens=[]\n",
    "            a = ''\n",
    "            is_ans = True\n",
    "            for i in range(len(ans_index)):\n",
    "                index = ans_index[i]\n",
    "                if ans_pred[index].item() == 1:\n",
    "                    a = text[index]\n",
    "                \n",
    "                    for j in range(1,4):\n",
    "                        #print(text[ans_index[i]],ans_index[i],ans_pred[ans_index[i]])\n",
    "                        if ans_pred[index+j].item() == 3: \n",
    "                            a += text[index+j]\n",
    "                            #print('1',a)\n",
    "                            break\n",
    "                        else:\n",
    "                            if index+j > MAX_LEN-3:\n",
    "                                is_ans = False\n",
    "                                break\n",
    "                            else:\n",
    "                                a += text[index+j]  \n",
    "                if is_ans:\n",
    "                    ans_tokens.append(a)\n",
    "            \n",
    "            \n",
    "            \n",
    "            names, answer = [], []       \n",
    "            if ans_tokens != []:\n",
    "                #print(ans_tokens)\n",
    "                ws_results = ws([text])\n",
    "                ckip_tokenized = list(set(ws_results[0]))\n",
    "                \n",
    "#             if ans_tokens != []:\n",
    "#                 print(''.join(text).replace('<pad>',''))\n",
    "#                 ner_results = nlp.ner(''.join(text).replace('<pad>',''))\n",
    "#                 for n in ner_results[0]:\n",
    "#                     if n[2] == 'PERSON' and len(n[3]) < 5:\n",
    "#                         names.append(n[3])\n",
    "\n",
    "            names = list(set(names))\n",
    "            \n",
    "            \n",
    "            for t in ans_tokens:\n",
    "                if t in ckip_tokenized and t not in answer and len(t) < 5 and len(t) > 1:\n",
    "                    if len(t) == 2 and t[1] in ['嫌','男','女','妻']:\n",
    "                        print('discard',t)\n",
    "                    elif len(t) == 4 and t[0]+t[1]+t[2] in names:\n",
    "                        print('discard',t)\n",
    "                    else:\n",
    "                        answer.append(t)\n",
    "                         \n",
    "                        \n",
    "            ids.append(newsid)\n",
    "            pred.append(answer)\n",
    "            print(f'news id : {newsid}, names : {answer}', end = '\\r')\n",
    "        prediction['news_ID'] = ids\n",
    "        prediction['name_pred'] = pred\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stanford_evaluation(MAX_LEN, tokenizer, test_loader,model):\n",
    "    prediction = pd.DataFrame(columns=['news_ID','name_pred'])\n",
    "    with torch.no_grad():\n",
    "        ids, pred = [], []\n",
    "        for data in test_loader: \n",
    "            input_ids =  [i.to(device) for i in data[1]][0]\n",
    "            input_ids = input_ids.unsqueeze(0)\n",
    "            newsid = data[0]\n",
    "            outputs = model(input_ids)\n",
    "            ans_pred = torch.sigmoid(outputs[0])\n",
    "            _, ans_pred = ans_pred.max(-1)\n",
    "            #print(ans_pred)\n",
    "            ans_pred = list(ans_pred[0])\n",
    "            #ans_index = [i for i,x in enumerate(ans_pred) if x ==2]\n",
    "            ans_index = [i for i in range(len(ans_pred)) if ans_pred[i] ==1]\n",
    "            #print(ans_index)\n",
    "            text = [tokenizer.convert_ids_to_tokens(i) for i in input_ids]\n",
    "            text = list(text[0])\n",
    "            ans_tokens = [text[x] for i,x in enumerate(ans_index)]\n",
    "            #ans_tokens = [text[ans_index[i]]+text[ans_index[i]+1] for i in range(len(ans_index)-1)]\n",
    "            \n",
    "            ans_tokens=[]\n",
    "            a = ''\n",
    "            is_ans = True\n",
    "            for i in range(len(ans_index)):\n",
    "                index = ans_index[i]\n",
    "                if ans_pred[index].item() == 1:\n",
    "                    a = text[index]\n",
    "                \n",
    "                    for j in range(1,4):\n",
    "                        #print(text[ans_index[i]],ans_index[i],ans_pred[ans_index[i]])\n",
    "                        if ans_pred[index+j].item() == 3: \n",
    "                            a += text[index+j]\n",
    "                            #print('1',a)\n",
    "                            break\n",
    "                        else:\n",
    "                            if index+j > MAX_LEN-3:\n",
    "                                is_ans = False\n",
    "                                break\n",
    "                            else:\n",
    "                                a += text[index+j]  \n",
    "                if is_ans:\n",
    "                    ans_tokens.append(a)\n",
    "            \n",
    "            names, answer = [], []       \n",
    "            if ans_tokens != []:\n",
    "                cleaned_text = ''.join(text).replace('<pad>','')\n",
    "                ner_results = nlp.ner(cleaned_text)\n",
    "                for n in ner_results:\n",
    "                    if n[1] == 'PERSON' and len(n[0]) < 5:\n",
    "                        names.append(n[0])\n",
    "            names = list(set(names))\n",
    "            \n",
    "            \n",
    "            for t in ans_tokens:\n",
    "                if t in names and t not in answer and len(t) < 5 and len(t) > 1:\n",
    "                    if len(t) == 2 and t[1] in ['嫌','男','女','妻']:\n",
    "                        print('discard',t)\n",
    "#                     elif len(t) == 4 and t[0]+t[1]+t[2] in names:\n",
    "#                         print('discard',t)\n",
    "                    else:\n",
    "                        answer.append(t)\n",
    "                         \n",
    "                        \n",
    "            ids.append(newsid)\n",
    "            pred.append(answer)\n",
    "            print(f'news id : {newsid}, names : {answer}', end = '\\r')\n",
    "        prediction['news_ID'] = ids\n",
    "        prediction['name_pred'] = pred\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = BertConfig.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "# model = BertForTokenClassification(config)       \n",
    "# model = model.cuda()\n",
    "# checkpoint = torch.load('bert_2label_6_best')\n",
    "# model.load_state_dict(checkpoint)\n",
    "# model = model.eval()\n",
    "# BS=1\n",
    "# test = NewsDataset('test',tokenizer, test_df)\n",
    "# testloader = torch.utils.data.DataLoader(test, batch_size=BS,shuffle=False)\n",
    "# prediction = evaluation(tokenizer, testloader,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "news id : ('T_072241',), names : ['陳文鶯', '陳文隆', '陳文宏', '黃崇庭']林祥曦', '陳俊哲']姜祖明', '謝春發', '歐兆賢', '蕭良政', '李智剛', '楊宇晨'] '陶浩志', '謝國清', '李宜珮']\r"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv('final_valid_0.9.csv')\n",
    "config = XLNetConfig.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "model = XLNetForTokenClassification(config)       \n",
    "model = model.cuda()\n",
    "checkpoint = torch.load('xlnet_660')\n",
    "model.load_state_dict(checkpoint)\n",
    "model = model.eval()\n",
    "MAX_LEN = 600\n",
    "BS=1\n",
    "test = NewsDataset('test',tokenizer, test_df)\n",
    "testloader = torch.utils.data.DataLoader(test, batch_size=BS,shuffle=False)\n",
    "prediction = evaluation(MAX_LEN,tokenizer, testloader,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "empty name list: 0.9852670349907919 ====== (counts: 535 / 543\n",
      "non empty name list: 0.95 ====== (counts: 133 / 140\n",
      "avg f1: 0.910305132918732\n",
      "total score: 656.0705826781904\n"
     ]
    }
   ],
   "source": [
    "true_pred = [ast.literal_eval(i) for i in test_df['name']]\n",
    "name_pred = prediction['name_pred'].to_list()\n",
    "avg_f1, nonempty, actual_nonempty, no_names, actual_nonames, total_score = get_score(name_pred,true_pred)\n",
    "print('empty name list:', no_names/actual_nonames, '====== (counts:',no_names, '/', actual_nonames)\n",
    "print('non empty name list:', nonempty/actual_nonempty, '====== (counts:',nonempty, '/', actual_nonempty)\n",
    "print('avg f1:', avg_f1)\n",
    "print('total score:', total_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction['name_true'] = test_df['name']\n",
    "prediction.to_csv('prediction_0720.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "empty name list: 0.9989082969432315 ====== (counts: 915 / 916\n",
      "non empty name list: 0.9836065573770492 ====== (counts: 60 / 61\n",
      "avg f1: 0.9494571332806625\n",
      "total score: 971.9674279968395\n"
     ]
    }
   ],
   "source": [
    "true_pred = [ast.literal_eval(i) for i in test_df['name']]\n",
    "name_pred = prediction['name_pred'].to_list()\n",
    "avg_f1, nonempty, actual_nonempty, no_names, actual_nonames, total_score = get_score(name_pred,true_pred)\n",
    "print('empty name list:', no_names/actual_nonames, '====== (counts:',no_names, '/', actual_nonames)\n",
    "print('non empty name list:', nonempty/actual_nonempty, '====== (counts:',nonempty, '/', actual_nonempty)\n",
    "print('avg f1:', avg_f1)\n",
    "print('total score:', total_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with ckip 斷詞\n",
      "empty name list: 0.9865841073271414 ====== (counts: 956 / 969\n",
      "non empty name list: 0.9753086419753086 ====== (counts: 158 / 162\n",
      "avg f1: 0.8988189832864651\n",
      "total score: 1098.0133993592622\n"
     ]
    }
   ],
   "source": [
    "print('with ckip 斷詞')\n",
    "true_pred = [ast.literal_eval(i) for i in test_df['name']]\n",
    "name_pred = prediction['name_pred'].to_list()\n",
    "avg_f1, nonempty, actual_nonempty, no_names, actual_nonames, total_score = get_score(name_pred,true_pred)\n",
    "print('empty name list:', no_names/actual_nonames, '====== (counts:',no_names, '/', actual_nonames)\n",
    "print('non empty name list:', nonempty/actual_nonempty, '====== (counts:',nonempty, '/', actual_nonempty)\n",
    "print('avg f1:', avg_f1)\n",
    "print('total score:', total_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with stanford ner\n",
      "empty name list: 0.9907120743034056 ====== (counts: 960 / 969\n",
      "non empty name list: 0.7777777777777778 ====== (counts: 126 / 162\n",
      "avg f1: 0.761055478912622\n",
      "total score: 1055.892990342991\n"
     ]
    }
   ],
   "source": [
    "print('with stanford ner')\n",
    "true_pred = [ast.literal_eval(i) for i in test_df['name']]\n",
    "name_pred = prediction['name_pred'].to_list()\n",
    "avg_f1, nonempty, actual_nonempty, no_names, actual_nonames, total_score = get_score(name_pred,true_pred)\n",
    "print('empty name list:', no_names/actual_nonames, '====== (counts:',no_names, '/', actual_nonames)\n",
    "print('non empty name list:', nonempty/actual_nonempty, '====== (counts:',nonempty, '/', actual_nonempty)\n",
    "print('avg f1:', avg_f1)\n",
    "print('total score:', total_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "去年\n",
      "九合一\n",
      "選舉\n",
      "因\n",
      "公投\n",
      "造成\n",
      "投\n",
      "開票\n",
      "延誤爭議\n",
      ",\n",
      "前\n",
      "中選會\n",
      "主委\n",
      "陳英鈐\n",
      "因而\n",
      "被\n",
      "國民黨\n",
      "及\n",
      "民眾\n",
      "告發\n",
      "「\n",
      "入\n",
      "聯\n",
      "公投\n",
      "」\n",
      "、\n",
      "「\n",
      "反\n",
      "核\n",
      "食\n",
      "重行\n",
      "公告\n",
      "」\n",
      "涉瀆職\n"
     ]
    }
   ],
   "source": [
    "sentence = '去年九合一選舉因公投造成投開票延誤爭議,前中選會主委陳英鈐因而被國民黨及民眾告發「入聯公投」、「反核食重行公告」涉瀆職'\n",
    "nerrr = nlp.ner(sentence)\n",
    "for n in nerrr:\n",
    "    print(n[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>pred</th>\n",
       "      <th>ans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 pred ans\n",
       "0           0   []  []"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peter_pred.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>news_ID</th>\n",
       "      <th>name_pred</th>\n",
       "      <th>name_true</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  news_ID name_pred name_true\n",
       "0           0        9        []        []"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jess_pred.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_name_list(x):\n",
    "    ret = [i for i in (x[1:-1]).replace(\"'\",'').split(', ')]\n",
    "    if ret == ['']:\n",
    "        return []\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "peter_pred = pd.read_csv('peter_pred.csv')\n",
    "jess_pred = pd.read_csv('prediction_0717.csv')\n",
    "\n",
    "j_p = jess_pred['name_pred'].map(get_name_list)\n",
    "p_p = peter_pred['pred'].map(get_name_list)\n",
    "t = jess_pred['name_true'].map(get_name_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_p = []\n",
    "for i in range(len(j_p)):\n",
    "    f_p.append( list( set(j_p[i]) | set( p_p[i] )))\n",
    "new_prediction = pd.DataFrame()\n",
    "new_prediction['pred'] = f_p\n",
    "new_prediction['true'] = t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "empty name list: 0.9989082969432315 ====== (counts: 915 / 916\n",
      "non empty name list: 1.0 ====== (counts: 61 / 61\n",
      "avg f1: 0.9279085692373007\n",
      "total score: 971.602422723475\n"
     ]
    }
   ],
   "source": [
    "avg_f1, nonempty, actual_nonempty, no_names, actual_nonames, total_score = get_score(new_prediction['pred'],new_prediction['true'])\n",
    "print('empty name list:', no_names/actual_nonames, '====== (counts:',no_names, '/', actual_nonames)\n",
    "print('non empty name list:', nonempty/actual_nonempty, '====== (counts:',nonempty, '/', actual_nonempty)\n",
    "print('avg f1:', avg_f1)\n",
    "print('total score:', total_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestingDataset(Dataset):\n",
    "    def __init__(self, MAX_LEN, tokenizer, data):\n",
    "        self.data = data\n",
    "        self.len = len(self.data)\n",
    "        self.tokenizer = tokenizer  \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    def clean_text(self,text):\n",
    "        text = text.replace('\\r','')\n",
    "        text = text.replace('\\n','')\n",
    "        return text\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        MAX_LEN = 600\n",
    "        content = self.data\n",
    "        text = self.clean_text(content)\n",
    "        inputs = self.tokenizer.encode_plus(text=text, max_length=MAX_LEN, return_tensors='pt', \n",
    "                                            pad_to_max_length = True, \n",
    "                                            return_token_type_ids = True,\n",
    "                                            return_attention_mask=True)\n",
    "        input_ids = inputs['input_ids'].squeeze(0)\n",
    "        segments_tensor = inputs['token_type_ids'].squeeze(0)\n",
    "        masks_tensor = inputs['attention_mask'].squeeze(0)\n",
    "        return input_ids\n",
    "    \n",
    "def evaluation(MAX_LEN, tokenizer, test_loader,model):\n",
    "    prediction = pd.DataFrame(columns=['news_ID','name_pred'])\n",
    "    with torch.no_grad():\n",
    "        ids, pred = [], []\n",
    "        for data in test_loader: \n",
    "            newsid, input_ids = [t.to(device) for t in data] \n",
    "            outputs = model(input_ids)\n",
    "            ans_pred = torch.sigmoid(outputs[0])\n",
    "            _, ans_pred = ans_pred.max(-1)\n",
    "            #print(ans_pred)\n",
    "            ans_pred = list(ans_pred[0])\n",
    "            #ans_index = [i for i,x in enumerate(ans_pred) if x ==2]\n",
    "            ans_index = [i for i in range(len(ans_pred)) if ans_pred[i] ==1 and ans_pred[i+1]==2]\n",
    "            #print(ans_index)\n",
    "            text = [tokenizer.convert_ids_to_tokens(i) for i in input_ids]\n",
    "            text = list(text[0])\n",
    "            ans_tokens = [text[x] for i,x in enumerate(ans_index)]\n",
    "            #ans_tokens = [text[ans_index[i]]+text[ans_index[i]+1] for i in range(len(ans_index)-1)]\n",
    "            \n",
    "            ans_tokens=[]\n",
    "            a = ''\n",
    "            for i in range(len(ans_index)):\n",
    "                index = ans_index[i]\n",
    "                if ans_pred[index].item() == 1:\n",
    "                    a = text[index]\n",
    "                \n",
    "                    for j in range(1,4):\n",
    "                        #print(text[ans_index[i]],ans_index[i],ans_pred[ans_index[i]])\n",
    "                        if ans_pred[index+j].item() == 3 or index > MAX_LEN-3:\n",
    "                            a += text[index+j]\n",
    "                            #print('1',a)\n",
    "                            break\n",
    "                        else:\n",
    "                            a += text[index+j]    \n",
    "                ans_tokens.append(a)\n",
    "                \n",
    "            names, answer = [], []\n",
    "            word_dict = dict()\n",
    "            if ans_tokens != []:\n",
    "                ws_results = ws([text])\n",
    "                pos_results = pos(ws_results)\n",
    "                ner_results = ner(ws_results, pos_results)\n",
    "                #count word frequency\n",
    "                for n in ner_results[0]:\n",
    "                    if n[2] == 'PERSON' and len(n[3]) > 1 and len(n[3]) < 5:\n",
    "                        names.append(n[3])\n",
    "                        if n[3] in list(word_dict.keys()):\n",
    "                            word_dict[n[3]] += 1 \n",
    "                        else: \n",
    "                            word_dict[n[3]] = 1 \n",
    "            \n",
    "            names = list(set(names))\n",
    "            #ckip parsing problem\n",
    "            parsed_names = []\n",
    "            for n in names:\n",
    "                parsed_names.append([x for x in n])\n",
    "            count = 0\n",
    "            for n in range(len(parsed_names)):\n",
    "                for m in range(len(parsed_names)):\n",
    "                    if n != m:\n",
    "                        n1 = parsed_names[n]\n",
    "                        n2 = parsed_names[m]\n",
    "                        if len(list(set(n1)&set(n2))) >= 2:\n",
    "                            n1_tf = word_dict[''.join(n1)]\n",
    "                            n2_tf = word_dict[''.join(n2)]\n",
    "                            if n1_tf >= n2_tf:\n",
    "                                if ''.join(n2) in names and len(n2) > 2 and n2_tf < 2:\n",
    "                                    names.remove(''.join(n2))\n",
    "                                    print('remove',''.join(n2),n1)\n",
    "                                    n += 1\n",
    "                            else:\n",
    "                                if ''.join(n1) in names and len(n2) > 2 and n1_tf < 2:\n",
    "                                    names.remove(''.join(n1))\n",
    "                                    print('remove',''.join(n1),n2)\n",
    "                                    n += 1\n",
    "                    if n > len(parsed_names)-1:\n",
    "                        break\n",
    "            \n",
    "            for t in ans_tokens:\n",
    "                for n in names:\n",
    "                    if t in n and n not in answer and len(n) < 5 and len(n) > 1:\n",
    "                        if len(n) == 2 and n[1] in ['嫌','男','女', '妻']:\n",
    "                            print('discard',n)\n",
    "                        elif len(n) == 4 and n[0]+n[1]+n[2] in names:\n",
    "                            print('discard',n)\n",
    "                        else:\n",
    "                            answer.append(n)\n",
    "                    \n",
    "            ids.append(newsid.item())\n",
    "            pred.append(answer)\n",
    "            print(f'news id : {newsid.item()}, names : {answer}', end = '\\r')\n",
    "        prediction['news_ID'] = ids\n",
    "        prediction['name_pred'] = pred\n",
    "    return prediction\n",
    "\n",
    "# config = XLNetConfig.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "# model = XLNetForTokenClassification(config)       \n",
    "# model = model.cuda()\n",
    "# checkpoint = torch.load('')\n",
    "# model.load_state_dict(checkpoint)\n",
    "# model = model.eval()\n",
    "# MAX_LEN = 600\n",
    "# BS=1\n",
    "# test = TestingDataset(MAX_LEN, tokenizer, test_df)\n",
    "# testloader = torch.utils.data.DataLoader(test, batch_size=BS,shuffle=False)\n",
    "# prediction = evaluation(MAX_LEN, tokenizer, testloader, model)\n",
    "# output = prediction['name_pred'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['王', '隆', '昌'] ['王', '隆', '昌', '是']\n",
      "1111 ['王', '隆', '昌'] ['王', '隆', '昌', '是'] 2 1 ['王隆昌', '吳淑珍', '王隆昌是']\n",
      "['王', '隆', '昌', '是'] ['王', '隆', '昌']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['王隆昌', '吳淑珍']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = ['王隆昌', '吳淑珍','王隆昌是']\n",
    "parsed_names = []\n",
    "for n in names:\n",
    "    parsed_names.append([x for x in n])\n",
    "\n",
    "word_dict = {'王隆昌':2, '吳淑珍':1, '王隆昌是':1}\n",
    "for n in range(len(parsed_names)):\n",
    "    for m in range(len(parsed_names)):\n",
    "        if n != m:\n",
    "            n1 = parsed_names[n]\n",
    "            n2 = parsed_names[m]\n",
    "            if len(list(set(n1)&set(n2))) >= 2:\n",
    "                print(n1,n2)\n",
    "                n1_tf = word_dict[''.join(n1)]\n",
    "                n2_tf = word_dict[''.join(n2)]\n",
    "                if n1_tf >= n2_tf:\n",
    "                    if ''.join(n2) in names:\n",
    "                        print('1111',n1,n2,n1_tf,n2_tf,names)\n",
    "                        names.remove(''.join(n2))\n",
    "                        n += 1\n",
    "                else:\n",
    "                    if ''.join(n1) in names:\n",
    "                        print('2222',n1,n2,n1_tf,n2_tf,names)\n",
    "                        names.remove(''.join(n1))\n",
    "                        n += 1\n",
    "        if n > len(parsed_names)-1:\n",
    "            break\n",
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['王隆昌', '吳淑珍']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33, 35, 'DATE', '本週')\n",
      "(63, 66, 'PERSON', '羅雅美')\n",
      "(73, 76, 'PERSON', '邱彰信')\n",
      "(236, 237, 'PERSON', '邱')\n",
      "(68, 71, 'ORG', '空品處')\n",
      "(173, 176, 'PERSON', '羅雅美')\n",
      "(10, 13, 'ORG', '國安局')\n",
      "(158, 162, 'WORK_OF_ART', '中國時報')\n",
      "(196, 198, 'DATE', '本週')\n",
      "(222, 224, 'DATE', '日前')\n",
      "(234, 235, 'PERSON', '羅')\n",
      "(237, 238, 'CARDINAL', '2')\n",
      "(177, 180, 'PERSON', '邱彰信')\n",
      "(231, 233, 'ORG', '華航')\n",
      "(92, 95, 'PERSON', '謝世謙')\n",
      "(151, 154, 'ORG', '國安局')\n",
      "(309, 313, 'ORG', 'TVBS')\n",
      "(145, 148, 'PERSON', '張恒嘉')\n",
      "(247, 250, 'PERSON', '謝世謙')\n",
      "(54, 56, 'ORG', '華航')\n",
      "(141, 144, 'PERSON', '吳宗憲')\n",
      "(27, 30, 'ORG', '國安局')\n",
      "(180, 181, 'CARDINAL', '2')\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      plate                 \n",
      "  ______|______              \n",
      " |             of           \n",
      " |             |             \n",
      " |            food          \n",
      " |       ______|_______      \n",
      " |      |          including\n",
      " |      |              |     \n",
      " A      |            fries  \n",
      " |      |              |     \n",
      "    delicious        French \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "from nltk import Tree\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "doc = nlp(' A plate of delicious food including French fries')\n",
    "\n",
    "def to_nltk_tree(node):\n",
    "    if node.n_lefts + node.n_rights > 0:\n",
    "        return Tree(node.orth_, [to_nltk_tree(child) for child in node.children])\n",
    "    else:\n",
    "        return node.orth_\n",
    "\n",
    "\n",
    "[to_nltk_tree(sent.root).pretty_print() for sent in doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      plate         \n",
      "  ______|________    \n",
      " |      |      found\n",
      " |      |        |   \n",
      " |      of       at \n",
      " |      |        |   \n",
      " |     food    diner\n",
      " |      |        |   \n",
      " A  disgusting   a  \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc =  nlp('A plate of disgusting food found at a diner')\n",
    "[to_nltk_tree(sent.root).pretty_print() for sent in doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import regex as re\n",
    "\n",
    "def count_overlapping(text, search_for):\n",
    "    return len(re.findall(search_for, text, overlapped=True))\n",
    "\n",
    "count_overlapping('陳思伃','陳思千')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['陳', '思']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista = [x for x in '陳思伃']\n",
    "listb = [x for x in '陳思思']\n",
    "list((set(lista) & set(listb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今天\n",
      "是\n",
      "星期一\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'今天': 1, '是': 1, '星期一': 1}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ws_results = ws(['今天是星期一'])\n",
    "word_dict = dict()\n",
    "for w in ws_results[0]:\n",
    "    if w not in list(word_dict.keys()):\n",
    "        word_dict[w] = 1 \n",
    "    else:\n",
    "        word_dict[w] += 1 \n",
    "    print(w)\n",
    "    \n",
    "word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-30-b3018622b60b>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-30-b3018622b60b>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    [x for x in enumerate names]\u001b[0m\n\u001b[0m                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "names = ['郭雅雯', '賴麗團', '林勇任']\n",
    "[x for x in enumerate names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jess",
   "language": "python",
   "name": "jess"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
